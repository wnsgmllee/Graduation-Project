<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Deepfake Detection, Audio-Visual">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Directionality-aware audio-visual deepfake detection considering cross-modal asymmetry</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">

          <h1 class="title is-1 publication-title">Directionality-aware Audio-Visual Deepfake Detection Considering Cross-modal Asymmetry</h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Jun Hee Lee<sup>1</sup>,</span>
            <span class="author-block">
              Jung Uk Kim†<sup>1</sup></span>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Computer Engineering, Kyung Hee University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./static/paper/Final_Report(Draft).pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/wnsgmllee/AVDFD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="./static/slides/Final_Presentation.pptx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-chalkboard-teacher"></i>
                  </span>
                  <span>Slides</span>
                </a>
              </span>

              <span class="link-block">
                <a href="./static/videos/Final_Presentation.mp4"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>Presentation</span>
                </a>
              </span>


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- =========================
     Framework (replaces all content before Abstract)
     - Place the image at: ./static/images/framework.jpg
     ========================= -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <figure class="image">
          <img src="./static/images/framework.png" alt="Overall framework">
        </figure>
        <p class="has-text-centered" style="margin-top: 0.75\rem;">
          Overall Framework
        </p>
      </div>
    </div>
  </div>
</section>


<!-- =========================
     Abstract (kept unchanged)
     ========================= -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            최근 오디오와 비주얼 정보를 함께 활용한 딥페이크 탐지 연구는 두 모달리티 간의 동기화 관계를 정량화하여 위조 여부를 판별하는 데 초점을 맞추어왔다. 이러한 접근은 주로 오디오와 비주얼을 대칭적인 관계로 가정하고, 시간 축상에서의 일대일 대응성을 강화하는 방식으로 학습이 이루어진다. 그러나 실제로 두 모달리티는 본질적으로 비대칭적인 특성을 가진다. 하나의 오디오는 특정한 입모양에만 대응하지만, 동일한 입모양이 여러 오디오 표현을 가질 수 있다. 기존 방법들은 이러한 오디오-시각 모달리티의 비대칭성을 고려하지 않아 학습 안정성과 일반화 성능이 저하되는 한계를 보인다. 따라서 본 연구는 이러한 문제를 해결하기 위해 오디오를 기준으로 시각 정보를 판별하는 방향성 기반 단방향 매핑 학습을 제안한다. 제안된 방법은 오디오 특성을 중심으로 해당 시각 정보가 실제인지 위조인지를 판별하도록 학습하며, 반대로 비주얼 정보를 통해 오디오를 추론하는 비효율적인 상호 의존 구조는 제거하였다. 또한 오디오 내부의 위조 단서를 정교하게 포착하기 위해 생성 분포에 기반한 재구성 오차 기반 잔차 학습을 도입하였다. 이를 통해 합성 오디오와 실제 오디오 간의 미세한 통계적 차이를 효과적으로 학습하도록 하였다. 추가로, 오디오 단서만으로도 위조 탐지가 가능하도록 학습하기 위해 화자 변환(voice conversion) 모델에 동일 화자를 reference로 입력하여 자기 화자 변환(self-conversion)을 수행한 동기화 유지 고난도 데이터를 구축하였다. 이를 통해 기존 모델들이 양방향 의존 관계에 빠지던 문제를 완화하고, 오디오-시각 모달리티의 비대칭성을 반영한 보다 강건한 탐지 모델을 구현하였다.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>


<!-- =========================
     Dataset section (structure as requested)
     - Text -> Video -> Text -> Figure
     ========================= -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Dataset</h2>



        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Replace with your figure path -->
            <figure class="image">
              <img src="./static/images/dataset.png" alt="Dataset figure">
            </figure>
            <p class="has-text-centered" style="margin-top: 0.75rem;">
              Dataset Figure
            </p>
          </div>
        </div>

        <div class="content has-text-justified">
          <p>
            본 연구에서는 오디오 중심 딥페이크 탐지 모델의 학습과 평가를 위해 기존 FakeAVCeleb 데이터셋을 기반으로 정제된 RealVideo–FakeAudio(Refined RVFA) 데이터를 새롭게 구축하였다. 기존 FakeAVCeleb의 RVFA 구성은 실제 영상에 다른 화자의 합성 음성을 결합한 형태로, 기본적인 오디오–시각 모달리티 불일치를 포함하고 있으나 합성 음성의 품질이 낮아 실제 발화 특성과는 다소 괴리가 있었다. 이로 인해 모델이 오디오 자체의 위조 단서보다는 비교적 쉽게 관측 가능한 음성–입 모양 간 동기화 오류에 과도하게 의존하는 문제가 발생하였다.

            이러한 한계를 보완하기 위해 본 연구에서는 화자 자기변환(voice conversion) 기반의 정밀 합성 절차를 도입하였다. 구체적으로 FakeAVCeleb의 RealVideo–RealAudio 쌍을 기준으로 설정하고, VoxCeleb2 데이터셋에서 동일 화자의 다른 발화를 참조 오디오로 선택하였다. 이후 Seed-VC 모델을 활용하여 원본 오디오의 시간적 구조, 즉 발화 타이밍과 길이, 강도는 그대로 유지하면서 음색만을 참조 화자의 특성으로 변환하였다. 이 과정으로 생성된 합성 음성은 발화 시점과 입 모양이 원본 영상과 정확히 일치하므로 시각적으로는 매우 자연스럽지만, 음향적으로는 새로운 화자 특성을 지닌 위조 음성에 해당한다.

            이와 같이 구축된 Refined RVFA 데이터셋은 높은 수준의 오디오–시각 동기화를 유지하면서도 합성 난이도가 크게 향상된 것이 특징이다. 시각적 단서만으로는 위조 여부를 판단하기 어려워, 탐지를 위해서는 오디오 신호 내부에 내재된 생성 흔적을 정밀하게 분석해야 한다. 따라서 본 데이터셋은 기존 탐지 모델이 동기화 단서에 얼마나 의존하고 있는지를 평가하고, 오디오 기반 위조 단서에 대한 강건성을 검증하는 데 적합한 실험 환경을 제공한다.

            실험에서는 제안된 오디오 중심 탐지 모델과 기존의 오디오–시각 동기화 기반 딥페이크 탐지 모델들을 동일한 학습 및 평가 프로토콜 하에서 비교하였다. 그 결과, Refined RVFA 환경에서 대부분의 기존 모델은 뚜렷한 성능 저하를 보인 반면, 제안된 오디오 중심 잔차 학습 모델은 높은 탐지 정확도를 안정적으로 유지하였다. 이는 본 연구의 접근 방식이 단순한 모달리티 정합 여부가 아니라, 오디오 생성 과정에서 발생하는 분포적 불일치를 효과적으로 학습하고 있음을 시사한다.
          </p>
        </div>


      </div>
    </div>
  </div>
</section>


<!-- =========================
     Experimental Results section (structure as requested)
     - Text -> Table image -> Text -> t-SNE image
     ========================= -->
<section class="section experimental-results">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Experimental Results</h2>

        <div class="content has-text-justified">
          <p>
            설명
          </p>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Replace with your table image path -->
            <figure class="image">
              <img src="./static/images/result1.png" alt="Results table">
            </figure>
            <p class="has-text-centered" style="margin-top: 0.75rem;">
              Results Table 1
            </p>
          </div>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Replace with your table image path -->
            <figure class="image">
              <img src="./static/images/result2.png" alt="Results table">
            </figure>
            <p class="has-text-centered" style="margin-top: 0.75rem;">
              Results Table 2
            </p>
          </div>
        </div>      
        

        <div class="content has-text-justified">
          <p>
            설명
          </p>
        </div>

        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <!-- Replace with your t-SNE image path -->
            <figure class="image">
              <img src="./static/images/result_tsne.png" alt="t-SNE visualization">
            </figure>
            <p class="has-text-centered" style="margin-top: 0.75rem;">
              t-SNE Visualization
            </p>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>


</body>
</html>
